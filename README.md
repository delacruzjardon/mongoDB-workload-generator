# Workload Generator for MongoDB

Workload Generator for MongoDB  was designed to help MongoDB users effortlessly generate data and simulate workloads for both sharded and non-sharded clusters. The generated workloads include standard CRUD operations, reflecting real-world usage patterns of a MongoDB environment.

Additionally, the tool supports the creation of realistic workloads by incorporating all CRUD operations through a set of queries that simulate common usage scenarios. Users can also define custom queries to run against collections created by the tool, further enhancing its flexibility and applicability.

While the tool provides extensive configuration capabilities, it requires minimal setup — only basic connection details are needed. It is user-friendly, does not require compilation, and offers a high level of flexibility.

The application was developed and tested using Python 3. As such, Python 3 is recommended for optimal compatibility. If upgrading is not feasible, modifications to the scripts may be necessary to support older Python versions.

The tool is optimized to leverage as many available CPU cores as you wish and supports configuration of an arbitrary number of threads among, enabling high parallelism and making it ideal for generating large-scale workloads and conducting effective stress tests on MongoDB clusters.


## Configuration

The tool consists of 4 files: 

* [mongodbCreds.py](mongodbCreds.py)
* [mongodbLoadQueries.py](mongodbLoadQueries.py) 
* [mongodbWorkload.py](mongodbWorkload.py)
* [app.py](app.py)

The only required configuration is in [Creds.py](Creds.py), where you define the connection details for your  cluster. The file is self-explanatory and includes examples to help you set up your environment correctly. You may also extend this file to include additional parameters as needed.

Custom queries can be defined in [mongodbLoadQueries.py](mongodbLoadQueries.py), but this is not a requirement. When adding new queries, ensure they target existing collections and fields generated by the tool to avoid runtime errors.

#### Pre-reqs

Workload Generator for MongoDB relies on a few additional Python libraries that are not included by default. To ensure proper functionality, please install the following dependencies:

- [faker](https://pypi.org/project/Faker/) – Used to generate random fictional data, ideal for bootstrapping databases, stress testing, and creating randomized datasets.
- [joblib](https://joblib.readthedocs.io/en/stable/) – Enables parallel execution of tasks by leveraging multiple CPU cores.
- [pymongo](https://www.mongodb.com/docs/languages/python/pymongo-driver/current/) – Library for interacting with MongoDB using Python.

To install these libraries, run the following command:

```
pip3 install faker joblib pymongo
```

## Functionality

By default, the workload runs for `60 seconds`, creating `4 threads` and using 1 CPU core. The workload creates a database called `airlines` with a collection named `flights_1` and populates it with documents containing data similar to the sample shown below. (Note: Sharding is not enabled by default.)

```
[direct: mongos] airlines> db.flights_1.findOne()
{
  _id: ObjectId('6806aafca42639c2279450a4'),
  flight_id: 6766976,
  flight_name: 'Flight_kopgt',
  departure: 'Crawfordside',
  arrival: 'Lake Tony',
  gate: 'K10',
  timestamp: ISODate('2025-04-21T16:30:52.512Z'),
  duration_minutes: 523,
  seats_available: 80,
  passengers: [
    { passenger_id: 1, name: 'Kenneth Gallagher', seat_number: '26F' },
    { passenger_id: 2, name: 'Lisa Moran', seat_number: '23B' },
    { passenger_id: 3, name: 'Mark Crosby', seat_number: '1E' },
    { passenger_id: 4, name: 'Mark Roy', seat_number: '22E' },
    { passenger_id: 5, name: 'Mark Nichols', seat_number: '12E' },
    { passenger_id: 6, name: 'Melissa Rivas', seat_number: '27A' },
    { passenger_id: 7, name: 'Andrew Bishop', seat_number: '17A' },
    { passenger_id: 8, name: 'Susan Williams', seat_number: '7F' },
    { passenger_id: 9, name: 'Shannon Cameron', seat_number: '13B' },
    { passenger_id: 10, name: 'Jean Rodriguez', seat_number: '12D' }
  ],
  equipment: {
    plane_type: 'Embraer E190',
    total_seats: 90,
    amenities: [ 'WiFi', 'TV', 'Power outlets' ]
  },
  flight_code: 'FLT-148'
}
```

The default query distribution ratio is as follows:

* 60% SELECT queries
* 20% UPDATE queries
* 10% INSERT queries
* 10% DELETE queries

This default distribution provides a balanced and meaningful baseline for performance testing and stress simulations. However, users are encouraged to adjust these ratios to better align with their specific use cases and workload characteristics.

During execution, the tool generates a real-time report every 5 seconds, showing the average number of queries executed across all utilized CPU cores, along with a detailed breakdown by operation type. At the end of the run, a final summary report is produced, providing statistics on overall workload performance and collection activity ([see sample output below](#Basic-Usage)).


## Usage

The workload is highly configurable at runtime, allowing you to adjust its behavior by modifying the parameters as needed. You can specify these parameters dynamically when executing the workload, as outlined below.

#### Getting help

You can customize various aspects of the workload, such as execution time, the number of threads, query distribution and more, by passing parameters during execution. You can obtain help and a list of all available parameters by running `./mongodbWorkload.py --help` 

```
./mongodbWorkload.py --help
usage: mongodbWorkload.py [-h] [--collection_name COLLECTION_NAME] [--collections COLLECTIONS] [--recreate] [--shard] [--runtime RUNTIME] [--batch_size BATCH_SIZE] [--threads THREADS] [--skip_update] [--skip_delete] [--skip_insert] [--skip_select] [--insert_ratio INSERT_RATIO] [--update_ratio UPDATE_RATIO] [--delete_ratio DELETE_RATIO] [--select_ratio SELECT_RATIO] [--report_interval REPORT_INTERVAL] [--cpu_ops] [--optimized] [--cpu CPU] [--log [LOG]]

Workload Generator for MongoDB

options:
  -h, --help                          show this help message and exit
  --collection_name COLLECTION_NAME   Name of the collection.
  --collections COLLECTIONS           How many collections to create (default 1).
  --recreate                          Recreate the collection before running the test.
  --shard                             Enable sharding on the collection.
  --runtime RUNTIME                   Duration of the load test, specify in seconds (e.g., 60s) or minutes (e.g., 5m) (default 60s).
  --batch_size BATCH_SIZE             Number of documents per batch insert (default 10).
  --threads THREADS                   Number of threads for simultaneous operations (default 4).
  --skip_update                       Skip update operations.
  --skip_delete                       Skip delete operations.
  --skip_insert                       Skip insert operations.
  --skip_select                       Skip select operations.
  --insert_ratio INSERT_RATIO         Percentage of insert operations (default 10).
  --update_ratio UPDATE_RATIO         Percentage of update operations (default 20).
  --delete_ratio DELETE_RATIO         Percentage of delete operations (default 10).
  --select_ratio SELECT_RATIO         Percentage of select operations (default 60).
  --report_interval REPORT_INTERVAL   Interval (in seconds) between workload stats output (default 5s).
  --cpu_ops                           Workload AVG OPS per CPU. This option enables real-time AVG OPS per CPU instead of CPU aggregate.
  --optimized                         Run optimized workload only. (default runs all workloads)
  --cpu CPU                           Number of CPUs to launch multiple instances of the workload in parallel (default 1).
  --log [LOG]                         Log filename and path (must be specified if --log is used). Default is off -- output is not logged to a file
```                        

#### Basic Usage 

Once you have configured the settings to match your environment, you can run the workload without specifying any parameters. This will utilize the default settings, providing a great way to familiarize yourself with the tool and review its output. The default setting is not optimized, this means it will randomly choose between slow and fast queries, so your performance may vary.

```
./mongodbWorkload.py
2025-03-31 10:35:24 - INFO -
===================================================================================================================
                                                 Workload Started
===================================================================================================================

Duration: 60 seconds
CPUs: 1
Threads: (Per CPU: 4 | Total: 4)
Collections: 1
Configure Sharding: False
Insert batch size: 10
Optimized workload: False
Workload ratio: SELECTS: 60% | INSERTS: 10% | UPDATES: 20% | DELETES: 10%
Report frequency: 5 seconds
Report logfile: None

2025-03-31 10:35:35 - INFO - AVG Operations last 5s (1 CPUs): 40.00 (SELECTS: 24.20, INSERTS: 4.00, UPDATES: 8.60, DELETES: 3.20)
2025-03-31 10:35:40 - INFO - AVG Operations last 5s (1 CPUs): 41.40 (SELECTS: 24.40, INSERTS: 4.20, UPDATES: 7.40, DELETES: 5.40)
2025-03-31 10:35:45 - INFO - AVG Operations last 5s (1 CPUs): 41.60 (SELECTS: 25.40, INSERTS: 3.80, UPDATES: 7.80, DELETES: 4.60)
2025-03-31 10:35:50 - INFO - AVG Operations last 5s (1 CPUs): 40.80 (SELECTS: 23.20, INSERTS: 4.40, UPDATES: 9.20, DELETES: 4.00)
2025-03-31 10:35:55 - INFO - AVG Operations last 5s (1 CPUs): 41.80 (SELECTS: 22.20, INSERTS: 4.60, UPDATES: 9.80, DELETES: 5.20)
2025-03-31 10:36:00 - INFO - AVG Operations last 5s (1 CPUs): 41.20 (SELECTS: 24.60, INSERTS: 3.60, UPDATES: 9.00, DELETES: 4.00)
2025-03-31 10:36:05 - INFO - AVG Operations last 5s (1 CPUs): 41.20 (SELECTS: 25.60, INSERTS: 4.40, UPDATES: 7.80, DELETES: 3.40)
2025-03-31 10:36:10 - INFO - AVG Operations last 5s (1 CPUs): 42.00 (SELECTS: 28.60, INSERTS: 3.20, UPDATES: 8.00, DELETES: 2.20)
2025-03-31 10:36:15 - INFO - AVG Operations last 5s (1 CPUs): 41.60 (SELECTS: 25.20, INSERTS: 3.40, UPDATES: 8.00, DELETES: 5.00)
2025-03-31 10:36:20 - INFO - AVG Operations last 5s (1 CPUs): 42.20 (SELECTS: 26.80, INSERTS: 4.00, UPDATES: 7.60, DELETES: 3.80)
2025-03-31 10:36:25 - INFO - AVG Operations last 5s (1 CPUs): 41.20 (SELECTS: 25.20, INSERTS: 4.40, UPDATES: 8.60, DELETES: 3.00)
2025-03-31 10:36:30 - INFO - AVG Operations last 5s (1 CPUs): 40.20 (SELECTS: 27.20, INSERTS: 5.20, UPDATES: 4.60, DELETES: 3.20)
2025-03-31 10:36:35 - INFO - AVG Operations last 5s (1 CPUs): 40.20 (SELECTS: 27.20, INSERTS: 5.20, UPDATES: 4.60, DELETES: 3.20)
2025-03-31 10:36:35 - INFO -
===================================================================================================================
                                                Workload Finished
===================================================================================================================

2025-03-31 10:36:36 - INFO -
================================================================================
|                               Collection Stats                               |
================================================================================
|            Name         |     Sharded      |      Size      |    Documents   |
================================================================================
|         flights_1       |      False       |    5.97 MB     |      2249      |
================================================================================

2025-03-31 10:36:36 - INFO -
===================================================================================================================
                                        Workload Stats (All CPUs Combined)
===================================================================================================================
Workload Runtime: 1.19 minutes
CPUs Used: 1
Total Operations: 2476 (SELECT: 1513, INSERT: 246, UPDATE: 482, DELETE: 235)
AVG QPS: 34.75 (SELECTS: 21.24, INSERTS: 3.45, UPDATES: 6.77, DELETES: 3.30)
Documents Inserted: 2460, Matching Documents Selected: 1229, Documents Updated: 364, Documents Deleted: 211
===================================================================================================================
```

#### Advanced Usage

You have a wide range of options available, and the parameters are neither exclusive nor mutually dependent, allowing you to use as many or as few as needed.

1. Collection Name
  - You can choose a different name for your collections. The default name is flights

2. Sharded Clusters

  - If you are running the workload against a sharded cluster, you can specify the `--shard` parameter to automatically enable sharding, create the necessary sharded indexes, and shard the collections. After the initial run, it is not necessary to include the `--shard` parameter in subsequent executions as long as you have not dropped the collections, as the collection will already be sharded (unless you drop the collection, then you'll need to start the workload again with `--shard`).
  - If you run a new workload after the initial workload has been run and create a new collection (see below) without the `--shard` option, the new collection will not be sharded, however, the previous 5 collections will.

3. Multiple collections

  - The workload will create multiple collections when you specify `--collections` with the desired number of collections, for example: `--collections 5`. Each collection will have its index count appended to its name, such as `flights_1`, `flights_2`, `flights_3`, and so on. You can create additional collections even after the initial workload has been run. For example, if you run the first workload with `--collections 5` and then run a new workload with `--collections 6`, a new collection will be created and the workload will run against all 6 of them.

4. Recreating collections

  - If you want to start from scratch you can pass `--recreate` and this will drop and recreate everything for you based on the parameters you provide. Keep in mind you need to provide `--shard` if you wish to shard the collections, as explained above.

5. Query ratio

  - You can adjust the default query ratio to suit your needs. For example, to run 90% SELECT statements instead of the default, use: `--select_ratio 90`. The workload will automatically distribute the remaining 10% across the other query types according to the original query ratio.

6. Workload duration

  - By default the workload runs for 60 seconds. You can chage this by specifying either seconds or minutes, e.g: `--runtime 15s`

7. Batch size

  - The workload performs its inserts in batches of 10 documents. You can change this behavior with a new batch size, e.g: `--batch_size 20`

8. Threads

  - By default, the workload uses 4 threads. This can be adjusted by specifying a new thread count, for example: `--threads 12`. This setting affects the client—the workload tool itself—and determines how many threads will be started. The more threads you use, the more load will be generated on the database.

9. CPUs

  - By default, the workload uses a single CPU. However, you can specify the number of CPUs to utilize for the workload, allowing for high parallelism. This setting, when combined with the thread configuration, can significantly increase traffic to the MongoDB cluster, leading to immense throughput. If you select a number of CPUs greater than the available amount, the workload will automatically configure itself to use the maximum available CPUs., e.g: `--cpu 10`. This setting affects the client—the workload tool itself—and determines how many CPUs will be utilized. The more CPUs you use, the more load you will be able to generate on the database.

10. Skipping certain workloads

  - You can tell the workload to skip certain workloads by using any of the following (these can be used in combination if needed):
    - `--skip_update`         
    - `--skip_delete`         
    - `--skip_insert`        
    - `--skip_select`         

11. Report interval (seconds)

  - You can configure the report inteval from the default 5s, e.g: `--report_interval 1`

12. Record the workload report to a log

  - You can configure the workload to log its output to a file. While the workload will continue to stream its output to the terminal, a log file will also be created for additional analysis, should you choose to enable this option. e.g: `--log /tmp/report.log`

13. Report per CPU stats
  - By default the real-time workload report outputs the stats for all CPUs combined however, you can change this behavior by providing `--cpu_ops`. This will change the default behavior of the real-time reporting to display the AVG OPS per CPU, instead of AVG OPS for all CPUs combined.    

14. Configure queries

  - You can add or remove queries from your workload by editing [mongodbLoadQueries.py](mongodbLoadQueries.py) to suit your requirements (there are a mix of fast and slow queries already provided). The queries for the workload are randomly chosen as the workload progresses, between aggregate (`collection.count_documents(query)`) and select queries (`collection.find(query, projection)`) and slow and fast, unless you use `--optimize`, which prevents the workload from choosing slow queries. 

15. Combine sharded and non-sharded collections in the same workload

  - You can create both sharded and non-sharded collections by following this workflow:
    1. Create a sharded workload by using the `--shard` parameter.
    2. Create another workload without specifying the `--shard` option, but increase the number of collections using `--collections` (make sure the number specified is higher than the current number of collections). For example, if you have 1 collection, you can use `--collections 2` and this would create 1 new collection that's not sharded.

  - This will utilize the existing sharded collections while creating new collections as non-sharded, since the `--shard` option was not provided, but the number of collections was increased with `--collections`.

16. Optimized Workload
  
  - This setting is disabled by default, allowing the workload to run all queries, including both optimized and ineffective queries. To run a more optimized workload, provide the `--optimized` argument when executing the workload. This ensures that queries use the flight ID for retrieval, rather than a range of flight IDs or other less optimal query patterns. Since the flight ID is indexed and serves as the shard key in sharded clusters, using it in queries improves performance and efficiency. Additionally, providing this flag enables find workloads to run only aggregate queries, reducing the overhead of sending records from the database to the client.

